{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les naufrag√©s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cher naufrag√©s, vous voici embarqu√©s pour une nouvelle aventure : d√©couvrir la datascience.  M√™me si dans la vraie vie, la page de d√©part est vierge, ici vous avez une trame qui vous guidera (et un mentor qui peut vous aider si vous √™tes perdus...)\n",
    "Le client est un utilisateurs Whatsapp qui a perdu ses contacts et veux r√©attribuer chaque message √† son auteur. On va donc utiliser la datasience pour pr√©dire l'auteur d'un message, en nous basant sur l'historique de la conversation des naufrag√©s.  \n",
    "**Au boulot !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premi√®re partie : ne pas r√©inventer la roue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaucoup de gens dans le monde ont d√©j√† travaill√© sur ces sujets et le grand principe en informatique, c'est d'√™tre flemmard et de r√©utiliser ce qui a d√©j√† √©t√© bien fait. Je commence donc par importer tous les outils, packages et autres bouts de codes qui nous serviront par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "# Model\n",
    "from transformers import CamembertTokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "# Viualisation\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deuxi√®me partie :  Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on va aller r√©cup√©rer les donn√©es et les standardiser.\n",
    "\n",
    "**En entr√©e :**  \n",
    "La conversation whatsapp en format texte (ficher .txt) qui contient les messages √† analyser.  \n",
    "Par exemple : _Message du 12 mars 9h53 de @0685294172 √† @06047412 : On se retrouve o√π ?_\n",
    "\n",
    "**En sortie :**  \n",
    "Un ensemble de vecteur math√©matiques -> chaque vecteur est un ensemble de nombre.\n",
    "Par exemple : \n",
    "\n",
    "- **message1**\n",
    "    - message1_nombre1\n",
    "    - message1_nombre2\n",
    "    ...\n",
    "    - message1_nombreN\n",
    "- **message2**\n",
    "    - message2_nombre1\n",
    "    - message2_nombre2\n",
    "    ...\n",
    "    - message2_nombreN\n",
    "    \n",
    "On va y aller pas √† pas..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 : Lire le fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La ligne suivante lis le fichier et le stocke dans une variable.\n",
    "full_text = open('../data/Discussion WhatsApp avec Les naufrag√©s üê†üèÑ_‚ôÄÔ∏èüèù.txt', \"r\", encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16/03/2020 √† 13:26 - Les messages envoy√©s dans ce groupe sont d√©sormais prot√©g√©s avec le chiffrement de bout en bout. Appuyez pour plus d\\'informations.\\n16/03/2020 √† 13:26 - \\u200eThomas Redoulez a cr√©√© le groupe \"Fuyons \"\\n16/03/2020 √† 13:26 - \\u200eThomas Redoulez vous a ajout√©\\n16/03/2020 √† 13:26 - \\u200eThomas Re'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Petit extrait juste pour voir √† quoi ca ressemble\n",
    "full_text[0:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Extraire les info importantes du fichier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut r√©cup√©rer le message et l'auteur du message. On va faire √ßa bien et r√©cup√©rer aussi l'heure et la date m√™me si on ne s'en servira pas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise des REGEX (Regular Expression). Il s'agit d'une syntaxe informatique particuli√®re qui sert √† faire des recherche dans du texte.  \n",
    "C'est affreux mais c'est tr√®s puissant.  \n",
    "L'exemple le plus connu c'est \".\" qui signifie \"n'importe quel caract√®re\". Un exemple plus utile : \"\\[a-z\\]\" qui veut dire \"n'importe quelle lettre en minuscule\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGEX\n",
    "pattern = r'^([\\d]{2}\\/[\\d]{2}\\/[\\d]{4}) √† ([\\d]{2}:[\\d]{2}) - ([^:]+)\\: (.*)$'\n",
    "tech_pattern = r'^([\\d]{2}\\/[\\d]{2}\\/[\\d]{4}) √† ([\\d]{2}:[\\d]{2}) -'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cr√©e un \"DataFrame\" que l'on apelle _conversation_. C'est un objet informatique qui contient un tableau. On va y ranger nos donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = pd.DataFrame(columns=['date', 'hour', 'author', 'message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On execute un _boucle for_ :  \n",
    "L'ordinateur va r√©p√©ter une instruction en boucle sur un ensemble d'√©l√©ments.  \n",
    "Dans chaque execution de la boucle, on d√©tecte la date, heure, auteur et message de la ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for msg in full_text.split('\\n'):\n",
    "    s = re.search(pattern, msg)\n",
    "    # petite astuce pour les plus perspicace : on ne range pas dans le tableau les messages qui ne \"collent\" pas au REGEX\n",
    "    if s is not None:\n",
    "        if i>0:\n",
    "            conversation.loc[i-1] = [date, hour, author, txt]\n",
    "        i += 1\n",
    "        date = s.group(1)\n",
    "        hour = s.group(2)\n",
    "        author = s.group(3)\n",
    "        txt = s.group(4)\n",
    "    # A la place on vient v√©rifier si c'est un message technique de whatsapp (changement d'icone, de nom de groupe)\n",
    "    else:\n",
    "        tech_s = re.search(tech_pattern, msg)\n",
    "        if tech_s is None: # Si ce n'est pas le cas, alors le message pr√©c√©dent √©tait sur plusieurs ligne\n",
    "            txt = txt + \"\\n\" + msg # On colle la nouvelle ligne √† l'ancienne\n",
    "\n",
    "conversation.loc[i-1] = [date, hour, author, txt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va voir √† quoi ressemble notre tableau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>author</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16/03/2020</td>\n",
       "      <td>13:42</td>\n",
       "      <td>Thomas Redoulez</td>\n",
       "      <td>RDV 15h30 Porte d'Italie √† la station d'essence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16/03/2020</td>\n",
       "      <td>15:00</td>\n",
       "      <td>Timoth√©e Guez</td>\n",
       "      <td>@33608138060 et @33616905893 quelle d√©cision p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16/03/2020</td>\n",
       "      <td>15:01</td>\n",
       "      <td>Cleophee Martinot Lagarde</td>\n",
       "      <td>C‚Äôest quoi l‚Äôadresse? 27 avenue de la Porte d‚Äô...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16/03/2020</td>\n",
       "      <td>15:02</td>\n",
       "      <td>Timoth√©e Guez</td>\n",
       "      <td>Exactement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16/03/2020</td>\n",
       "      <td>15:05</td>\n",
       "      <td>Thomas Redoulez</td>\n",
       "      <td>Oui !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>22/04/2020</td>\n",
       "      <td>23:06</td>\n",
       "      <td>Marie-Gabriel Bourgue</td>\n",
       "      <td>Sympa vos marques de bronzage ;) üí™üèΩüí™üèΩüí™üèΩ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>22/04/2020</td>\n",
       "      <td>23:18</td>\n",
       "      <td>Diane De Sentenac</td>\n",
       "      <td>&lt;M√©dias omis&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>23/04/2020</td>\n",
       "      <td>11:39</td>\n",
       "      <td>Diane De Sentenac</td>\n",
       "      <td>&lt;M√©dias omis&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>23/04/2020</td>\n",
       "      <td>11:39</td>\n",
       "      <td>Diane De Sentenac</td>\n",
       "      <td>&lt;M√©dias omis&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>23/04/2020</td>\n",
       "      <td>14:41</td>\n",
       "      <td>Marion Cadart</td>\n",
       "      <td>force et honneur @33661099325 üí™üèΩ\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>662 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   hour                     author  \\\n",
       "0    16/03/2020  13:42            Thomas Redoulez   \n",
       "1    16/03/2020  15:00              Timoth√©e Guez   \n",
       "2    16/03/2020  15:01  Cleophee Martinot Lagarde   \n",
       "3    16/03/2020  15:02              Timoth√©e Guez   \n",
       "4    16/03/2020  15:05            Thomas Redoulez   \n",
       "..          ...    ...                        ...   \n",
       "657  22/04/2020  23:06      Marie-Gabriel Bourgue   \n",
       "658  22/04/2020  23:18          Diane De Sentenac   \n",
       "659  23/04/2020  11:39          Diane De Sentenac   \n",
       "660  23/04/2020  11:39          Diane De Sentenac   \n",
       "661  23/04/2020  14:41              Marion Cadart   \n",
       "\n",
       "                                               message  \n",
       "0     RDV 15h30 Porte d'Italie √† la station d'essence.  \n",
       "1    @33608138060 et @33616905893 quelle d√©cision p...  \n",
       "2    C‚Äôest quoi l‚Äôadresse? 27 avenue de la Porte d‚Äô...  \n",
       "3                                           Exactement  \n",
       "4                                                Oui !  \n",
       "..                                                 ...  \n",
       "657            Sympa vos marques de bronzage ;) üí™üèΩüí™üèΩüí™üèΩ  \n",
       "658                                      <M√©dias omis>  \n",
       "659                                      <M√©dias omis>  \n",
       "660                                      <M√©dias omis>  \n",
       "661                 force et honneur @33661099325 üí™üèΩ\\n  \n",
       "\n",
       "[662 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Standardiser les donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce moment l√†, on utilise un projet de l'INRIA : CamemBERT. C'est un projet qui a analys√© des centaines de Gb de textes fran√ßais pour fournir une mani√®re intelligente de transformer les mots en vecteurs. Ils ont obtenus de bons r√©sultats, et on en profite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 121, 11, 660, 16, 730, 25543, 110, 6]\n",
      "[5, 13532, 76, 11, 660, 16, 5271, 6]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base') # tokenizer est le convertisseur mot <-> vecteurs\n",
    "# Juste pour avoir un exemple...\n",
    "print(tokenizer.encode(\"J'aime le camembert\"))\n",
    "print(tokenizer.encode(\"Vraiment j'aime le fromage\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour info, les d√©but et fin de messages sont toujours encod√© \"5\" et \"6\".  \n",
    "On fait le m√™me travail avec les noms des auteurs, qui doivent √™tre transform√©s en chiffres. L√† aussi un outil existe d√©j√†."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder() # le (pour label encoder) est le convertisseur etiquette <-> nom d'auteur\n",
    "le.fit(conversation.author) # Il faut lui fournir la liste des √©tiquettes pour qu'il sache comment convertir..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cr√©√© la fonction qu'on va appliquer au tableau.  \n",
    "**Entr√©e :** message au format texte  \n",
    "**Sortie :** Vecteur (ensemble de nombre)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentence(msg):\n",
    "    l = len(tokenizer) # l est la taille du dictionnaire CamemBERT\n",
    "    # On transforme le msg en chiffres\n",
    "    sparse_vectors = tokenizer.encode(msg) \n",
    "    # On passe de '3' √† [0,0,1,0,0...]\n",
    "    vec = np.zeros(l)\n",
    "    for sparse_vec in sparse_vectors:        \n",
    "        vec[sparse_vec] = vec[sparse_vec] + 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a maintenant tous les outils pour cr√©er notre X, qui contiendra l'ensemble de nos donn√©es standardis√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (838 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "X = np.array(len(conversation)) # Cr√©ation de X vide\n",
    "l = list()\n",
    "for i, row in conversation.iterrows():\n",
    "     # On applique la fonction √† chaque ligne et on stocke le r√©sultat dans une liste\n",
    "    l.append(transform_sentence(row['message']))\n",
    "X = np.array(l) # On place la liste dans X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cr√©e aussi le Y avec les √©tiquettes : les auteurs de chaque message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = le.transform(conversation.author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fin de la transformation de donn√©es. On a maintenant X et Y, parfaitement structur√©s pour rentrer dans les algorithmes classiques de datascience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troisi√®me partie : modelisation de donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie sera rapide.  \n",
    "J'ai choisi un algorithme assez classique qui s'appelle SVC (Support Vector  Clustering). Il n'y a pas besoin de le r√©√©crire : tout est pr√™t √† l'emploi.  \n",
    "\n",
    "Le choix de l'algorithme est une question passionnante mais pas imm√©diate. C'est complexe et on commence g√©n√©ralement par faire quelque chose de simple mais qui marche, avant de se poser cette question.  \n",
    "\n",
    "En avant pour la version \"simple\" donc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma=0.15)\n",
    "clf.fit(X, Y) # Et hop, un simple ligne et l'apprentissage est fini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observons le r√©sultat :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.inverse_transform(clf.predict([transform_sentence(\"C'est pas faux\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quatri√®me partie : visualisation de donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va essayer de visualiser le top 20 des mots les plus utilis√©s par notre fan de Kaamelott.  \n",
    "Pour √ßa, on commence par pr√©dire l'ensemble de nos donn√©es :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nouvelle _boucle for_ :  \n",
    "Pour chaque auteur, on s√©lectionne les message qui ont √©t√© predits comme √©tant de lui.  \n",
    "\n",
    "Ensuite on additionne tous les mots et on stocke le tout dans une variable \"atavisme\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atavism = {} # vide pour l'instant\n",
    "for i in np.unique(predictions): # pour chaque auteur\n",
    "    words_dict = {}\n",
    "    personal_words = X[predictions == i] # personal_words contient l'ensemble des mots utilis√©s par l'auteur\n",
    "    bag_of_words = personal_words.sum(axis=0) # on somme les vecteurs : mot 1 + mot 1 + mot 3 = [2, 0, 1]\n",
    "    \n",
    "    for token in bag_of_words.argsort()[-20:][::-1]: # On fait le top 20\n",
    "        word = tokenizer.decode(int(token), skip_special_tokens=True)\n",
    "        words_dict[word] = bag_of_words[token]\n",
    "    del words_dict['<s>'] # on supprime les d√©but de message -> code 5\n",
    "    del words_dict['</s>']# on supprime les fin de message -> code 6\n",
    "    atavism[str(le.inverse_transform([i])[0])] = words_dict # On ajoute le r√©sultat √† la variable atavism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moment tant attendu : on visualise le tout !  \n",
    "\n",
    "On fait un \"nuage de mots\". Ce format est tr√®s utilis√© pour les textes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = 'Marion Cadart'\n",
    "wc = WordCloud(background_color=\"black\",width=1000,height=1000, max_words=20,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(atavism[author])\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cinqui√®me partie : √† vous de jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que c'est tout nul. Devinez-vous pourquoi ? Comment corriger ? \n",
    "R√©ponse en cliquant sur les 3 petit points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "On a laiss√© beaucoup de \"parasites\" : \n",
    "- la ponctuation\n",
    "- les \"m√©dias omis\" quand on envoie une photo sur whatsapp. \n",
    "- ...  \n",
    "On va corriger √ßa en ajoutant  \n",
    "```conversation = conversation[conversation['message'] != '<M√©dias omis>'].reset_index()```  \n",
    "```conversation['message'] = conversation['message'].map(lambda x: re.sub(r'[\\W^\\s]+', ' ', x))```  \n",
    "au preprocessing des donn√©es.\n",
    "Que font ces lignes ? O√π les mettre ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et si vous √©chouez, rappelez vous que c'est dans le th√®me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shipwecked",
   "language": "python",
   "name": "shipwecked"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
